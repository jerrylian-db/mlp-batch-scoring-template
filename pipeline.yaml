# `pipeline.yaml` is the main configuration file for an MLflow Pipeline.
# Required pipeline parameters should be defined in this file with either concrete values or
# variables such as {{ INGEST_DATA_LOCATION }}.

# Variables must be dereferenced in a profile YAML file, located under `profiles/`.
# See `profiles/local.yaml` for example usage. One may switch among profiles quickly by
# providing a profile name such as `local` in the Pipeline object constructor:
# `p = Pipeline(profile="local")`
#
# NOTE: YAML does not support tabs for indentation. Please use spaces and ensure that all YAML
# files are properly formatted.

template: "batch_scoring/v1"
# Specifies the dataset to score
data:
  # Dataset locations on the local filesystem are supported, as well as HTTP(S) URLs and
  # any other remote locations resolvable by MLflow, such as those listed in
  # https://mlflow.org/docs/latest/tracking.html#artifact-stores
  location: {{INGEST_DATA_LOCATION}}
  # Beyond `parquet` datasets, the `spark_sql` and `delta` formats are also natively supported for
  # use with Spark
  format: {{INGEST_DATA_FORMAT|default('parquet')}}
  # Datasets with other formats, including `csv`, can be used by implementing and
  # specifying a `custom_loader_method`
  custom_loader_method: steps.ingest.load_file_as_dataframe
  # If the `spark_sql` `format` is specified, the `sql` entry is used to specify a SparkSQL
  # statement that identifies the dataset to use
  sql: SELECT * FROM delta.`{{INGEST_DATA_LOCATION}}`
  # If the `delta` `format` is specified, you can also configure the Delta table `version` to read
  # or the `timestamp` at which to read data
  # version: 2
  # timestamp: 2022-06-01T00:00:00.000Z
steps:
  preprocessing:
    preprocess_method: steps.preprocessing.process_data
  predict:
    # The model URI for the model used to scored the input dataset. See the list of accepted
    # model URIs here:
    # https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf
    model_uri: "models/model.pkl"
    # There are three available output formats:
    # - `parquet` for writing to parquet file.
    # - `delta` for writing to a delta file.
    # - `table` for saving the scored dataframe as a Spark table.
    # Note that overwrites are not allowed at this time.
    output_format: {{OUTPUT_DATA_FORMAT|default('parquet')}}
    # For the `parquet` and `delta` `output_format`s, use the `output_location` parameter to specify
    # your desired path and file name (relative to where you run the pipeline).
    # For the `table` `output_format`, use the `output_location` parameter to specify your desired
    # Spark table name.
    output_location: {{OUTPUT_DATA_LOCATION}}
